# -*- coding: utf-8 -*-
"""Linear Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJ42AHajdbrO_IYZ1rGUWON-ERNQd_s1

# Part 1 - Model Training

This code looks at creating a linear regression model, looking at reproducing Moore's Law.
"""

# Commented out IPython magic to ensure Python compatibility.
# We first verify the correct version of TF installed
# !pip install -q tensorflow-gpu==2.1.0

try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

import tensorflow as tf
print(tf.__version__)
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Obtaining the data - this is done using the wget method, to obtain the data from the instructor's github repo

!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv

# We then load the data, and split it into train data and labels, and reshape X to have it in the format accepted by Keras/Tensorflow
# We also check the shapes of X and y, to make sure they follow the required format

data = pd.read_csv('moore.csv', header=None).values
X = data[:,0]
print(X.shape)
X.reshape(-1,1)
print(X.shape)
y = data[:,1]
print(y.shape)

# To further check the data, we plot it
# In a scatter plot, it is exponential. If we take the log of y, the data follows a linear pattern.

plt.scatter(X,y)

plt.figure()
y_log = np.log(y)
plt.scatter(X,y_log)

# Currently, X is the year. For creating the model, we need to center X around 0
# This is done so that the values are not too large, as currently the values are large 
# This can either be done by scaling, but this means that latter the data would need to be reverse transformed.
# Scaluing would also mean that a step increase in X would no longer be equivalent to a 1 year increase in real terms.

X = X - X.mean()

# Now, the Tensofrlow model is created. An input and dense layer are created.
# For linear regression, there is no activation function.
# Two models are created:
# - the first one uses the adam optimizer
# - the second one uses an optimizer object, with learning rate = 0.001, and momentum = 0.9 (this allows the user more control)
# The mean squared error (mse) is used as a loss function
# A learning rate scheduler is also created, to scale the learning rate based on the training epoch.

model_1 = tf.keras.models.Sequential(
    [
     tf.keras.layers.Input(shape=(1,)),
     tf.keras.layers.Dense(1)
    ]
)

model_1.compile(optimizer = 'adam', loss='mse')

model_2 = tf.keras.models.Sequential(
    [
     tf.keras.layers.Input(shape=(1,)),
     tf.keras.layers.Dense(1)
    ]
)

model_2.compile(optimizer = tf.keras.optimizers.SGD(0.001, 0.9), loss='mse')

def schedule(epoch, learning_rate):
  """This function schedules a variable learning rate based on the epoch number

  :param epoch: The current training epoch
  :type epoch: float
  :return: A value for the learning rate
  :rtype: float
  """
  if epoch >= 50:
    return 0.0001
  else:
    return 0.001

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)

r1 = model_1.fit(X, y_log, epochs = 200, callbacks=[scheduler])
r2 = model_2.fit(X, y_log, epochs = 200, callbacks=[scheduler])

# To assess how the models perfored, we plot their loss per iteration.

plt.figure()
plt.plot(r1.history['loss'], label='loss adam')
plt.legend()

plt.figure()
plt.plot(r2.history['loss'], label='loss sgd')
plt.legend()

# To better understand the model, we would like to get the slope of the line, which is related to the doublind rate of the transistor count
# To get the trained weights of the model, we need to access the relevant layer.
# We can observe the input layer is a dummy-layer, keeping track of the input size.
# The '.get_weights' returns two arrays, representing W and b.

print(model_2.layers)
print(model_2.layers[0].get_weights())
print("The Slope:", model_2.layers[0].get_weights()[0][0,0])

print("Time to double:", np.log(2) / model_2.layers[0].get_weights()[0][0,0])

# To verify the above calculation, the analytical solution of linear regression is also calculated.

X = np.array(X).flatten()
Y = np.array(y_log)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)

